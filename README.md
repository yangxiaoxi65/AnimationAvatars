# Animation Avatars

A pipeline for generating animatable avatars using in-the-wild video sequence for Brown CSCI 1430 Computer Vision course

Tianhao Shi, Xiaoxi Yang, Hayden McDonald, Nicholas Marsano

## Requirements

- 64-bit Linux
- NVIDIA GPU with CUDA 12.x 
- Conda support

## Installation

1. Clone the repository

```bash
git clone --recursive https://github.com/yangxiaoxi65/AnimationAvatars.git
```

2. Setup environment

```bash
cd AnimationAvatars
conda env create -f environment.yml
conda activate animation_avatars
```
3. Install OpenPose and build it from source


Due to the extreme finickiness of OpenPose, we refer to the [original repository](https://github.com/CMU-Perceptual-Computing-Lab/openpose.git) for installation instructions.

After installing, make sure to add the path to `openpose.bin` to your `PATH` environment variable. For example,

```bash
which openpose.bin # /path/to/openpose/build/examples/openpose/openpose.bin
export PATH=$PATH:/path/to/openpose/build/examples/openpose
```

4. Download SAM Model Weight (**NOT SAM 2**)

While recursive cloning should have added the SAM repository, you need to download the model weights separately. Be sure to download the `sam_vit_h_4b8939.pth` [here](https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints) file and place it in the `AnimationAvatars/segmet-and-animate/checkpoints`.

5. Download SMPL files and put them in the `assets` folder under name `smpl_files`

You need to register and download [SMPL](https://smpl.is.tue.mpg.de/) and [SMPL-X](https://smpl-x.is.tue.mpg.de/) files. 

The file sturcture should look like this:

```bash
assets/smpl_files
├── smpl
│   ├── SMPL_FEMALE.pkl
│   ├── SMPL_MALE.pkl
│   └── SMPL_NEUTRAL.pkl
└── smplx
    ├── SMPLX_FEMALE.npz
    ├── SMPLX_MALE.npz
    └── SMPLX_NEUTRAL.npz
```

6. == TODO: Mention setup form Smimplify-X==

## Running the pipeline

### Stage 1 + 2: Openpose + SAM
We have provideded a script to run stage 1 and stage 2 of the pipeline, called `run_stage_1_2.py`. Create a directory called `assets` in the root directory and place a video under this folder. Then:
```bash
conda activate cv_final
python run_stage_1_2.py assets/{video_file}
```

This should generate a folder in `assets` with the same name as the video file. Inside this folder, there should be the following folders:
```bash
assets/{video_name}
├── cameras.npz     # generated dummy camera parameters
├── images          # the frames extracted from the video
├── keypoints.npy   # openpose output in numpy format
├── mask_images     # visualization of the masks generated by SAM
├── masks           # masks generated by SAM
├── openpose_images # openpose output visualized
└── openpose_json   # openpose output in json format
```

### Stage 3: SMPLify-X
TODO: Add instructions for running stage 3


Optionally, you can use `refine-smpl.py` to refine the SMPL parameters. If you decide not to do so, just copy and rename the `poses.npz` file in the `assets/{video_name}` folder to `poses_optimized.npz`. 

### Stage 4: Gaussian Splatting Training
We adopted a fork of the Gaussian Avatar repository with fixed bugs. The conda environment we provide should be sufficient to run the code. However, if you encounter any issues, please see their README and install a new environment with the required packages.

You should also download all their requred files into the `assets` folder inside `Gaussian Avatar`.

TL;DR: 
```bash
cd GaussianAvatar
conda env create -f environment.yml
conda activate gaussian_avatar
```

Then we need to convert to their desired training file structure:

```bash
cd scripts
python sample_romp2gsavatar.py --data_folder ../../assets/{video_name}
python gen_pose_map_cano_smpl.py {video_name}
```

Finally, we can train it!
```bash
cd .. # to the root of Gaussian Avatar
python train.py -s ../assets/{video_name} -m ../output/{video_name} --train_stage 1 --pose_op_start_iter 10 --save_epoch 10 --epochs 400 # we find that for a video with ~600 frames, it is better to train at least 400 epochs
```

### Stage 5: Pose Reconstruction / Novel Pose Rendering
To conduct pose reconstruction, run:
```bash
cd GaussianAvatar
python render_pose_reconstruct.py -s ../assets/{video_name} -m ../output/{video_name} --epoch 400
```
You should see a folder called `pose_reconstruct` in the `output/{video_name}` folder. This folder contains the reconstructed poses for each frame in the video.


Suppose you have multiple videos that went throgh the first 3 stages of the pipeline, i.e, you have a `poses.npz` (or `poses_optimized.npz`) file for each video. You can use the following script to render the novel poses for each video. 
```bash
cd GaussianAvatar
python render_novel_pose.py -s ../assets/{video_name} -m ../output/{video_name} --epoch 400 --pose_file {path_to_poses.npz}
```
This will generate a folder called `novel_pose` in the `output/{video_name}` folder. This folder contains the rendered novel poses for each frame in the video.


Please let us know if you encounter any issues. Contact [Tianhao Shi](tianhao_shi@brown.edu) for troubles with the pipeline.

## Known Issues

You cannot run the `gen_pose_map_cano_smpl.py` script without a monitor. Also we haven't verified running it on Mac or Windows.
